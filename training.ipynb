{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Training a Sensitive Information NER Model using Auto-Annotated Clinical Notes\n",
    "\n",
    "This script auto-annotates clinical notes from the `SYNTHETIC_TEXT` column in your CSV.\n",
    "Sensitive information is marked by placeholders of the form `[** ... **]`. We use custom\n",
    "heuristics to label the text inside these placeholders:\n",
    "- If the content matches a date pattern (YYYY-MM-DD), label as DATE.\n",
    "- If the content matches a phone pattern (e.g., 555-0109), label as PHONE.\n",
    "- If the content contains \"hospital\" (case-insensitive), label as LOCATION.\n",
    "- If the content is a numeric range (e.g., 5-9), label as NUMERIC.\n",
    "- Otherwise, label as NAME.\n",
    "Tokens are then annotated in a BIO scheme (e.g., B-DATE, I-DATE).\n",
    "\n",
    "After annotation, the data is converted into a Hugging Face Dataset, tokenized (with label alignment)\n",
    "and used to fine-tune a pre-trained model (`BioBERT` in this example) for token classification.\n",
    "\n",
    "Dependencies:\n",
    "- pandas\n",
    "- nltk\n",
    "- transformers\n",
    "- datasets\n",
    "- scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (3.3.2)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.13/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.29.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: 0.26.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers torch\n",
    "%pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1742236051050
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/areef/Desktop/Drexel/Winter 25/Robust Deep Learning/Final Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /Users/areef/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1742236051226
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Module 1: Data Loading & Preprocessing\n",
    "# ----------------------------\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads the CSV file with clinical notes from the column SYNTHETIC_TEXT.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype={4: str, 5: str})\n",
    "        print(\"Number of notes loaded:\", len(df))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error loading dataset:\", str(e))\n",
    "        exit(1)\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"For training annotation we leave the placeholders intact and just strip extra whitespace.\"\"\"\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame, text_column: str = \"SYNTHETIC_TEXT\") -> pd.DataFrame:\n",
    "    df['TEXT_PRE'] = df[text_column].apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1742236051477
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tokens: ['Admission', 'Date', ':', '1914-12-13', 'Discharge', 'Date', ':', '1952-09-09', 'Date', 'of', 'Birth', ':', 'Sex', ':', 'F', 'Service', ':', 'MICU', 'and', 'then', 'to', 'Thompson', 'Medicine', 'HISTORY', 'OF', 'PRESENT', 'ILLNESS', ':', 'This', 'is', 'an', '81-year-old', 'female', 'with', 'a', 'history', 'of', 'emphysema', '(', 'not', 'on', 'home', 'O2', ')', ',', 'who', 'presents', 'with', 'three', 'days', 'of', 'shortness', 'of', 'breath', '.', 'Presented', 'to', 'the', 'County', 'Hospital', 'Emergency', 'Room', '.', 'Followup', 'with', 'Dr', '.', 'Jackson', 'at', '555-0109', '.']\n",
      "Sample Labels: ['O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'O', 'B-PHONE', 'O']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Module 2: Auto-Annotation of Sensitive Tokens\n",
    "# ----------------------------\n",
    "def infer_label(placeholder_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Infers a sensitive label from the content inside a placeholder.\n",
    "    Heuristics:\n",
    "      - If content matches a date pattern (YYYY-MM-DD), return \"DATE\".\n",
    "      - If content matches a phone pattern (e.g., 555-0109), return \"PHONE\".\n",
    "      - If content matches a numeric range (e.g., 5-9), return \"NUMERIC\".\n",
    "      - If content contains \"hospital\" (case-insensitive), return \"LOCATION\".\n",
    "      - Otherwise, return \"NAME\".\n",
    "    \"\"\"\n",
    "    content = placeholder_content.strip()\n",
    "    if re.match(r\"^\\d{4}-\\d{1,2}-\\d{1,2}$\", content):\n",
    "        return \"DATE\"\n",
    "    if re.match(r\"^\\d{3}-\\d{4}$\", content):\n",
    "        return \"PHONE\"\n",
    "    if re.match(r\"^\\d+-\\d+$\", content):\n",
    "        return \"NUMERIC\"\n",
    "    if \"hospital\" in content.lower():\n",
    "        return \"LOCATION\"\n",
    "    return \"NAME\"\n",
    "\n",
    "def annotate_text(text: str):\n",
    "    \"\"\"\n",
    "    Auto-annotates a clinical note.\n",
    "    Sensitive information is assumed to be enclosed within [** ... **].\n",
    "    Tokens inside such a span are annotated with a BIO scheme using the label\n",
    "    inferred from the content. Tokens outside are labeled \"O\".\n",
    "    \n",
    "    Returns:\n",
    "        tokens (list[str]), ner_tags (list[str])\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    last_index = 0\n",
    "    # Find placeholders\n",
    "    for match in re.finditer(r\"\\[\\*\\*(.*?)\\*\\*\\]\", text):\n",
    "        start, end = match.span()\n",
    "        sensitive_content = match.group(1)\n",
    "        label = infer_label(sensitive_content)\n",
    "        # Annotate tokens before the sensitive span as non-sensitive.\n",
    "        before = text[last_index:start]\n",
    "        tokens_before = nltk.word_tokenize(before)\n",
    "        for token in tokens_before:\n",
    "            annotations.append((token, \"O\"))\n",
    "        # Annotate sensitive tokens using BIO scheme.\n",
    "        sensitive_tokens = nltk.word_tokenize(sensitive_content)\n",
    "        for i, token in enumerate(sensitive_tokens):\n",
    "            if i == 0:\n",
    "                annotations.append((token, f\"B-{label}\"))\n",
    "            else:\n",
    "                annotations.append((token, f\"I-{label}\"))\n",
    "        last_index = end\n",
    "    # Annotate any remaining tokens after the last placeholder.\n",
    "    after = text[last_index:]\n",
    "    tokens_after = nltk.word_tokenize(after)\n",
    "    for token in tokens_after:\n",
    "        annotations.append((token, \"O\"))\n",
    "    tokens = [token for token, tag in annotations]\n",
    "    ner_tags = [tag for token, tag in annotations]\n",
    "    return tokens, ner_tags\n",
    "\n",
    "def convert_csv_to_ner_dicts(csv_path: str):\n",
    "    \"\"\"\n",
    "    Converts the CSV file into a list of dictionaries with keys:\n",
    "      \"id\", \"tokens\", \"ner_tags\".\n",
    "    Each row is a complete clinical note.\n",
    "    \"\"\"\n",
    "    df = load_data(csv_path)\n",
    "    ner_data = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = row[\"SYNTHETIC_TEXT\"]\n",
    "        tokens, tags = annotate_text(text)\n",
    "        ner_data.append({\n",
    "            \"id\": i,\n",
    "            \"tokens\": tokens,\n",
    "            \"ner_tags\": tags\n",
    "        })\n",
    "    return ner_data\n",
    "\n",
    "# Test annotation on a sample note.\n",
    "sample_note = \"\"\"Admission Date: [**1914-12-13**]       Discharge Date: [**1952-09-09**]\n",
    "\n",
    "Date of Birth:                    Sex:  F\n",
    "\n",
    "Service:  MICU and then to [**Thompson**] Medicine\n",
    "\n",
    "HISTORY OF PRESENT ILLNESS:  This is an 81-year-old female with a history of emphysema (not on home O2), who presents with three days of shortness of breath. Presented to the [**County Hospital**] Emergency Room. Followup with Dr. [**Jackson**] at [**555-0109**].\"\"\"\n",
    "tokens, ner_tags = annotate_text(sample_note)\n",
    "print(\"Sample Tokens:\", tokens)\n",
    "print(\"Sample Labels:\", ner_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1742236051750
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Module 3: Tokenization and Label Alignment for Training\n",
    "# ----------------------------\n",
    "\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define our label list (using BIO scheme).\n",
    "label_list = [\"O\", \"B-DATE\", \"I-DATE\", \"B-NAME\", \"I-NAME\", \"B-LOCATION\", \"I-LOCATION\", \"B-NUMERIC\", \"I-NUMERIC\", \"B-PHONE\", \"I-PHONE\"]\n",
    "num_labels = len(label_list)\n",
    "label_to_id = { label: i for i, label in enumerate(label_list) }\n",
    "id_to_label = { i: label for i, label in enumerate(label_list) }\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 padding=\"max_length\",\n",
    "                                 max_length=128,\n",
    "                                 is_split_into_words=True)\n",
    "    all_labels = []\n",
    "    for i, words in enumerate(examples[\"tokens\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        labels = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                label = examples[\"ner_tags\"][i][word_idx]\n",
    "                if word_idx == previous_word_idx and label.startswith(\"B-\"):\n",
    "                    label = \"I-\" + label[2:]\n",
    "                labels.append(label_to_id[label])\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1742236342861
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes loaded: 11940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9552/9552 [00:37<00:00, 258.02 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2388/2388 [00:09<00:00, 257.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Module 4: Prepare the Dataset for Training\n",
    "# ----------------------------\n",
    "\n",
    "ner_data = convert_csv_to_ner_dicts(\"data/SYNTHETIC_DISCHARGE_REPORTS.csv\")\n",
    "dataset = Dataset.from_list(ner_data)\n",
    "dataset_dict = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1742236355015
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/areef/Desktop/Drexel/Winter 25/Robust Deep Learning/Final Project/.venv/lib/python3.13/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/zx/m536dpgs2rsbxy_j8_w52kmc0000gn/T/ipykernel_20106/321766935.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1791' max='1791' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1791/1791 13:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.002975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.002027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Module 5: Training the NER Model\n",
    "# ----------------------------\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_checkpoint_v2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=50,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # Implement token-level precision/recall/F1 here if desired.\n",
    "    return {}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model(\"model_v2\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
